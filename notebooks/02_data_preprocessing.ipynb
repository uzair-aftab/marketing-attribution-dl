{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ”§ Data Preprocessing: Criteo Attribution Dataset\n",
                "\n",
                "This notebook prepares the Criteo data for attribution modeling by:\n",
                "1. Creating user journey sequences\n",
                "2. Engineering features (time deltas, position encoding, attention masks)\n",
                "3. Building train/validation/test splits\n",
                "4. Saving processed data for downstream models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch: 2.6.0+cu124\n",
                        "Device: cuda\n",
                        "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from pathlib import Path\n",
                "import pickle\n",
                "from tqdm import tqdm\n",
                "import warnings\n",
                "import json\n",
                "from datetime import datetime\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Check GPU availability\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"Device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "# Paths\n",
                "DATA_RAW = Path('../data/raw/criteo')\n",
                "DATA_PROCESSED = Path('../data/processed')\n",
                "DATA_PROCESSED.mkdir(exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Full Dataset\n",
                "\n",
                "The Criteo dataset has ~16M rows. We'll use a configurable sample size."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Criteo Attribution Dataset...\n",
                        "Sample size: 8,000,000 rows\n",
                        "\n",
                        "Loaded 8,000,000 impressions\n",
                        "Unique users: 3,759,156\n",
                        "Unique campaigns: 675\n",
                        "Memory usage: 1.41 GB\n"
                    ]
                }
            ],
            "source": [
                "# Configuration - adjust based on your system resources\n",
                "# With 32GB RAM, you can handle 8-10M rows comfortably\n",
                "SAMPLE_ROWS = 8_000_000  # Increased from 5M for better model quality\n",
                "\n",
                "print(\"Loading Criteo Attribution Dataset...\")\n",
                "print(f\"Sample size: {SAMPLE_ROWS:,} rows\")\n",
                "\n",
                "df = pd.read_csv(\n",
                "    DATA_RAW / 'criteo_attribution_dataset.tsv.gz', \n",
                "    sep='\\t', \n",
                "    nrows=SAMPLE_ROWS\n",
                ")\n",
                "\n",
                "print(f\"\\nLoaded {len(df):,} impressions\")\n",
                "print(f\"Unique users: {df['uid'].nunique():,}\")\n",
                "print(f\"Unique campaigns: {df['campaign'].nunique():,}\")\n",
                "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Data Overview ---\n",
                        "Columns: ['timestamp', 'uid', 'campaign', 'conversion', 'conversion_timestamp', 'conversion_id', 'attribution', 'click', 'click_pos', 'click_nb', 'cost', 'cpo', 'time_since_last_click', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\n",
                        "\n",
                        "Conversion rate: 4.8223%\n",
                        "Click rate: 35.2893%\n",
                        "\n",
                        "Timestamp range: 0 to 1256682\n"
                    ]
                }
            ],
            "source": [
                "# Quick data overview\n",
                "print(\"\\n--- Data Overview ---\")\n",
                "print(f\"Columns: {list(df.columns)}\")\n",
                "print(f\"\\nConversion rate: {df['conversion'].mean():.4%}\")\n",
                "print(f\"Click rate: {df['click'].mean():.4%}\")\n",
                "print(f\"\\nTimestamp range: {df['timestamp'].min()} to {df['timestamp'].max()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Create User Journey Sequences\n",
                "\n",
                "We create fixed-length sequences per user with:\n",
                "- Campaign IDs (encoded)\n",
                "- Click indicators\n",
                "- Costs\n",
                "- Time deltas (globally normalized)\n",
                "- Position encoding\n",
                "- Attention masks (for padding)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unique campaigns: 675\n",
                        "Campaign ID range: 0 to 674\n"
                    ]
                }
            ],
            "source": [
                "# Sort by user and timestamp\n",
                "df = df.sort_values(['uid', 'timestamp'])\n",
                "\n",
                "# Encode campaigns\n",
                "campaign_encoder = LabelEncoder()\n",
                "df['campaign_encoded'] = campaign_encoder.fit_transform(df['campaign'])\n",
                "\n",
                "print(f\"Unique campaigns: {len(campaign_encoder.classes_)}\")\n",
                "print(f\"Campaign ID range: 0 to {len(campaign_encoder.classes_) - 1}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Computing global time delta statistics...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Computing stats: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3759156/3759156 [00:59<00:00, 63233.63it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Time delta mean: 169292.26\n",
                        "Time delta std: 214311.97\n"
                    ]
                }
            ],
            "source": [
                "# Compute global time delta statistics for normalization\n",
                "print(\"Computing global time delta statistics...\")\n",
                "all_time_deltas = []\n",
                "for uid, group in tqdm(df.groupby('uid'), desc='Computing stats'):\n",
                "    timestamps = group['timestamp'].values\n",
                "    if len(timestamps) > 1:\n",
                "        deltas = np.diff(timestamps)\n",
                "        all_time_deltas.extend(deltas[deltas > 0].tolist())\n",
                "\n",
                "TIME_DELTA_MEAN = np.mean(all_time_deltas) if all_time_deltas else 1.0\n",
                "TIME_DELTA_STD = np.std(all_time_deltas) if all_time_deltas else 1.0\n",
                "print(f\"Time delta mean: {TIME_DELTA_MEAN:.2f}\")\n",
                "print(f\"Time delta std: {TIME_DELTA_STD:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Creating sequences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3759156/3759156 [15:47<00:00, 3968.16it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Created 3,759,156 user sequences\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Configuration\n",
                "MAX_SEQ_LEN = 20  # Maximum impressions per journey\n",
                "\n",
                "def create_user_sequences(df, max_len=MAX_SEQ_LEN):\n",
                "    \"\"\"Create fixed-length sequences for each user with all required features.\"\"\"\n",
                "    sequences = []\n",
                "    seq_lengths = []  # Track for statistics\n",
                "    \n",
                "    for uid, group in tqdm(df.groupby('uid'), desc='Creating sequences'):\n",
                "        group = group.sort_values('timestamp')\n",
                "        \n",
                "        # Get user's journey data (last N impressions)\n",
                "        campaigns = group['campaign_encoded'].values[-max_len:]\n",
                "        timestamps = group['timestamp'].values[-max_len:]\n",
                "        clicks = group['click'].values[-max_len:]\n",
                "        costs = group['cost'].values[-max_len:]\n",
                "        \n",
                "        # User-level targets\n",
                "        converted = int(group['conversion'].max())\n",
                "        total_cost = float(group['cost'].sum())\n",
                "        \n",
                "        # Original sequence length (before padding)\n",
                "        seq_len = len(campaigns)\n",
                "        seq_lengths.append(seq_len)\n",
                "        \n",
                "        # Pad sequences if needed\n",
                "        if seq_len < max_len:\n",
                "            pad_len = max_len - seq_len\n",
                "            campaigns = np.pad(campaigns, (pad_len, 0), constant_values=0)  # 0 = padding\n",
                "            timestamps = np.pad(timestamps, (pad_len, 0), constant_values=0)\n",
                "            clicks = np.pad(clicks, (pad_len, 0), constant_values=0)\n",
                "            costs = np.pad(costs, (pad_len, 0), constant_values=0)\n",
                "        \n",
                "        # Calculate time deltas (globally normalized)\n",
                "        time_deltas = np.diff(timestamps, prepend=timestamps[0])\n",
                "        time_deltas = (time_deltas - TIME_DELTA_MEAN) / (TIME_DELTA_STD + 1e-8)\n",
                "        time_deltas = np.clip(time_deltas, -10, 10)  # Clip outliers\n",
                "        \n",
                "        # Create attention mask (1 = real token, 0 = padding)\n",
                "        mask = np.zeros(max_len, dtype=np.float32)\n",
                "        mask[-seq_len:] = 1.0  # Real tokens are at the end after padding\n",
                "        \n",
                "        # Position encoding (relative positions)\n",
                "        positions = np.arange(max_len, dtype=np.int64)\n",
                "        \n",
                "        # Campaigns +1 so 0 can be padding index in embeddings\n",
                "        campaigns = campaigns + 1\n",
                "        \n",
                "        sequences.append({\n",
                "            'uid': uid,\n",
                "            'campaigns': campaigns.astype(np.int64),\n",
                "            'clicks': clicks.astype(np.float32),\n",
                "            'costs': costs.astype(np.float32),\n",
                "            'time_deltas': time_deltas.astype(np.float32),\n",
                "            'positions': positions,\n",
                "            'mask': mask,\n",
                "            'seq_len': min(seq_len, max_len),\n",
                "            'converted': converted,\n",
                "            'total_cost': total_cost\n",
                "        })\n",
                "    \n",
                "    return sequences, seq_lengths\n",
                "\n",
                "sequences, seq_lengths = create_user_sequences(df)\n",
                "print(f\"\\nCreated {len(sequences):,} user sequences\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Sequence Length Distribution ---\n",
                        "Min: 1\n",
                        "Max: 20\n",
                        "Mean: 2.11\n",
                        "Median: 1\n",
                        "Sequences with length = MAX_SEQ_LEN: 9,200\n"
                    ]
                }
            ],
            "source": [
                "# Sequence length statistics\n",
                "print(\"\\n--- Sequence Length Distribution ---\")\n",
                "print(f\"Min: {min(seq_lengths)}\")\n",
                "print(f\"Max: {max(seq_lengths)}\")\n",
                "print(f\"Mean: {np.mean(seq_lengths):.2f}\")\n",
                "print(f\"Median: {np.median(seq_lengths):.0f}\")\n",
                "print(f\"Sequences with length = MAX_SEQ_LEN: {sum(1 for s in seq_lengths if s >= MAX_SEQ_LEN):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train/Validation/Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Split Sizes ---\n",
                        "Train: 2,631,409 (70.0%)\n",
                        "Val:   563,873 (15.0%)\n",
                        "Test:  563,874 (15.0%)\n",
                        "\n",
                        "--- Conversion Rates (should be similar across splits) ---\n",
                        "Train: 5.0185%\n",
                        "Val:   5.0185%\n",
                        "Test:  5.0187%\n"
                    ]
                }
            ],
            "source": [
                "# Stratified split to maintain conversion rate distribution\n",
                "labels = [s['converted'] for s in sequences]\n",
                "\n",
                "# 70% train, 15% val, 15% test\n",
                "train_seq, temp_seq = train_test_split(\n",
                "    sequences, \n",
                "    test_size=0.3, \n",
                "    random_state=42, \n",
                "    stratify=labels\n",
                ")\n",
                "\n",
                "temp_labels = [s['converted'] for s in temp_seq]\n",
                "val_seq, test_seq = train_test_split(\n",
                "    temp_seq, \n",
                "    test_size=0.5, \n",
                "    random_state=42, \n",
                "    stratify=temp_labels\n",
                ")\n",
                "\n",
                "print(\"--- Split Sizes ---\")\n",
                "print(f\"Train: {len(train_seq):,} ({len(train_seq)/len(sequences):.1%})\")\n",
                "print(f\"Val:   {len(val_seq):,} ({len(val_seq)/len(sequences):.1%})\")\n",
                "print(f\"Test:  {len(test_seq):,} ({len(test_seq)/len(sequences):.1%})\")\n",
                "\n",
                "print(\"\\n--- Conversion Rates (should be similar across splits) ---\")\n",
                "print(f\"Train: {sum(s['converted'] for s in train_seq) / len(train_seq):.4%}\")\n",
                "print(f\"Val:   {sum(s['converted'] for s in val_seq) / len(val_seq):.4%}\")\n",
                "print(f\"Test:  {sum(s['converted'] for s in test_seq) / len(test_seq):.4%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Save Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… Saved to ..\\data\\processed\n",
                        "\n",
                        "--- Config ---\n",
                        "  n_campaigns: 676\n",
                        "  max_seq_len: 20\n",
                        "  sample_rows: 8000000\n",
                        "  n_train: 2631409\n",
                        "  n_val: 563873\n",
                        "  n_test: 563874\n",
                        "  conversion_rate: 0.050185\n",
                        "  time_delta_mean: 169292.264153\n",
                        "  time_delta_std: 214311.971348\n",
                        "  created_at: 2025-12-15T05:05:43.889145\n",
                        "  feature_dim: 4\n"
                    ]
                }
            ],
            "source": [
                "# Comprehensive config for reproducibility\n",
                "config = {\n",
                "    'n_campaigns': len(campaign_encoder.classes_) + 1,  # +1 for padding token\n",
                "    'max_seq_len': MAX_SEQ_LEN,\n",
                "    'sample_rows': SAMPLE_ROWS,\n",
                "    'n_train': len(train_seq),\n",
                "    'n_val': len(val_seq),\n",
                "    'n_test': len(test_seq),\n",
                "    'conversion_rate': sum(labels) / len(labels),\n",
                "    'time_delta_mean': float(TIME_DELTA_MEAN),\n",
                "    'time_delta_std': float(TIME_DELTA_STD),\n",
                "    'created_at': datetime.now().isoformat(),\n",
                "    'feature_dim': 4  # campaigns, clicks, costs, time_deltas\n",
                "}\n",
                "\n",
                "# Save sequences\n",
                "processed_data = {\n",
                "    'train_sequences': train_seq,\n",
                "    'val_sequences': val_seq,\n",
                "    'test_sequences': test_seq,\n",
                "    'config': config\n",
                "}\n",
                "\n",
                "with open(DATA_PROCESSED / 'processed_sequences.pkl', 'wb') as f:\n",
                "    pickle.dump(processed_data, f)\n",
                "\n",
                "# Save encoders\n",
                "encoders = {\n",
                "    'campaign': campaign_encoder\n",
                "}\n",
                "with open(DATA_PROCESSED / 'encoders.pkl', 'wb') as f:\n",
                "    pickle.dump(encoders, f)\n",
                "\n",
                "# Save config as JSON for easy inspection\n",
                "with open(DATA_PROCESSED / 'config.json', 'w') as f:\n",
                "    json.dump(config, f, indent=2)\n",
                "\n",
                "print(f\"\\nâœ… Saved to {DATA_PROCESSED}\")\n",
                "print(f\"\\n--- Config ---\")\n",
                "for k, v in config.items():\n",
                "    if isinstance(v, float):\n",
                "        print(f\"  {k}: {v:.6f}\")\n",
                "    else:\n",
                "        print(f\"  {k}: {v}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Verifying Saved Data ---\n",
                        "Train sequences loaded: 2,631,409\n",
                        "Val sequences loaded: 563,873\n",
                        "Test sequences loaded: 563,874\n",
                        "\n",
                        "Sample sequence keys: ['uid', 'campaigns', 'clicks', 'costs', 'time_deltas', 'positions', 'mask', 'seq_len', 'converted', 'total_cost']\n",
                        "Campaigns shape: (20,)\n",
                        "Mask shape: (20,)\n"
                    ]
                }
            ],
            "source": [
                "# Verify saved data\n",
                "print(\"\\n--- Verifying Saved Data ---\")\n",
                "with open(DATA_PROCESSED / 'processed_sequences.pkl', 'rb') as f:\n",
                "    loaded = pickle.load(f)\n",
                "\n",
                "print(f\"Train sequences loaded: {len(loaded['train_sequences']):,}\")\n",
                "print(f\"Val sequences loaded: {len(loaded['val_sequences']):,}\")\n",
                "print(f\"Test sequences loaded: {len(loaded['test_sequences']):,}\")\n",
                "\n",
                "# Sample sequence structure\n",
                "sample = loaded['train_sequences'][0]\n",
                "print(f\"\\nSample sequence keys: {list(sample.keys())}\")\n",
                "print(f\"Campaigns shape: {sample['campaigns'].shape}\")\n",
                "print(f\"Mask shape: {sample['mask'].shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "âœ… PREPROCESSING COMPLETE!\n",
                        "============================================================\n",
                        "\n",
                        "Ready for modeling with 2,631,409 training sequences\n",
                        "\n",
                        "Files created:\n",
                        "  - ..\\data\\processed\\processed_sequences.pkl\n",
                        "  - ..\\data\\processed\\encoders.pkl\n",
                        "  - ..\\data\\processed\\config.json\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"âœ… PREPROCESSING COMPLETE!\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nReady for modeling with {len(train_seq):,} training sequences\")\n",
                "print(f\"\\nFiles created:\")\n",
                "print(f\"  - {DATA_PROCESSED / 'processed_sequences.pkl'}\")\n",
                "print(f\"  - {DATA_PROCESSED / 'encoders.pkl'}\")\n",
                "print(f\"  - {DATA_PROCESSED / 'config.json'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "**Next:** [03_rule_based_models.ipynb](03_rule_based_models.ipynb) - Implement baseline attribution models"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "transformers-gpu-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
